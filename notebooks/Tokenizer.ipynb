{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Tokenization\n",
    "\n",
    "### BertTweet\n",
    "\n",
    "- fastBPE\n",
    "- 64K subword\n",
    "\n",
    "### Twilbert\n",
    "- SentencePiece (fastBPE)\n",
    "- 30k subword "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from glob import glob\n",
    "\n",
    "num_files = 100\n",
    "tweet_files = glob(\"../data/filtered_tweets/*.txt\")\n",
    "\n",
    "train_files = tweet_files[:2]\n",
    "\n",
    "\n",
    "tweets = list([x.strip(\"\\n\") for x in open(\"../data/filtered_tweets/spanish-tweets-000.txt\")])[:100_000]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "len(tweets)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "from tokenizers import SentencePieceUnigramTokenizer, SentencePieceBPETokenizer, BertWordPieceTokenizer\n",
    "\n",
    "tokenizer = SentencePieceBPETokenizer()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "from finetune_vs_scratch.preprocessing import special_tokens\n",
    "from finetune_vs_scratch.tokenizer import tokenizer_special_tokens\n",
    "\n",
    "#tokenizer.add_special_tokens(tokenizer_special_tokens)\n",
    "tokenizer.train_from_iterator(\n",
    "    tweets,\n",
    "    vocab_size=30_000,\n",
    "    min_frequency=5,\n",
    "    show_progress=True,\n",
    "    special_tokens=tokenizer_special_tokens,\n",
    "    limit_alphabet=500,\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "\n",
    "vocab = tokenizer.get_vocab()\n",
    "inv_vocab = {v:k for k, v in vocab.items()}\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "vocab[\"@usuario\"]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "525"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "\n",
    "tokenizer.encode(\"QuÃ© hacesssss @usuario\").tokens"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['â–QuÃ©', 'â–haces', 'ss', 'ss', 'â–@usuario']"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "\n",
    "\n",
    "\" \".join([inv_vocab[i] for i in range(1000)])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'<s> </s> <unk> <pad> <mask> ! \" # $ % & \\' ( ) * + , - . / 0 1 2 3 4 5 6 7 8 9 : ; = ? @ A B C D E F G H I J K L M N O P Q R S T U V W X Y Z [ \\\\ ] ^ _ ` a b c d e f g h i j k l m n o p q r s t u v w x y z { | } Â¡ Â¿ Ã€ Ã Ãˆ Ã‰ Ã Ã‘ Ã“ Ã˜ Ãš Ãœ Ã  Ã¡ Ã¢ Ã£ Ã¤ Ã¥ Ã§ Ã¨ Ã© Ãª Ã¬ Ã­ Ã¯ Ã± Ã² Ã³ Ã´ Ãµ Ã¶ Ã¸ Ã¹ Ãº Ã¼ Å Éœ Éª É´ É¾ Ê€ Ê Ê” Ê• Ê Ë Ë¶ Ì Ì„ Ì† Ì“ ÌŸ Ì¡ Ì£ Ì¤ Ì³ Ì¶ Î± Î¹ Ï Ï‰ Ğ” Ğ° Ğµ Ğ· Ğ¸ Ñ‚ × Ø± Ù„ Ù… Ùˆ ÙŠ Ù© Û¶ à²¥ à¸… à¹‘ áƒ§ áƒ« áƒ® á„ á…² á¥† á¥‰ á¥’ á¥™ á¥£ á¥± á¥² á¥´ á´€ á´… á´‡ á´ á´– á´— á´˜ á´› á´œ \\u200b \\u200d â„ \\u2066 \\u2069 âƒ£ â†‘ â†’ â†“ â‡’ âˆ‡ âˆ§ â‰¦ â‰§ âŠ™ âŒ“ â± â”€ â” â”ƒ â” â”“ â”— â”› â• â•‘ â•— â•š â•­ â•® â– â–ˆ â–¡ â–ª â–¶ â–º â–½ â—‡ â—‹ â— â˜€ â˜… â˜† â˜‘ â˜Ÿ â™ª â™« âš  âœ âœ âœ” â£ â¡ â € â¬† â¬‡ â¬ˆ ã€ ã€‚ ã€Œ ã€ ã€ ã€ ã€ ã€‘ ã€£ ã‚ ã„ ã† ãˆ ãŠ ã‹ ãŒ ã ã ã‘ ã“ ã” ã• ã— ã™ ã› ã ãŸ ã  ã¡ ã£ ã¤ ã¥ ã¦ ã§ ã¨ ã© ãª ã« ã® ã¯ ã² ã³ ã¾ ã¿ ã‚ ã‚‚ ã‚„ ã‚ˆ ã‚‰ ã‚Š ã‚‹ ã‚Œ ã‚ ã‚ ã‚’ ã‚“ ã‚™ ã‚š ã‚¡ ã‚¢ ã‚£ ã‚¤ ã‚¦ ã‚§ ã‚ª ã‚« ã‚­ ã‚¯ ã‚° ã‚³ ã‚µ ã‚· ã‚¸ ã‚¹ ã‚º ã‚¿ ãƒ ãƒƒ ãƒ„ ãƒˆ ãƒ‰ ãƒŠ ãƒ‹ ãƒ ãƒ ãƒ‘ ãƒ• ãƒ— ãƒ™ ãƒœ ãƒ ãƒŸ ãƒ  ãƒ¡ ãƒ¢ ãƒ£ ãƒ¥ ãƒ¦ ãƒ© ãƒª ãƒ« ãƒ¬ ãƒ­ ãƒ³ ãƒ» ãƒ¾ ä¸€ ä¸‰ ä¸­ äºˆ äºº ä»Š ä»˜ ä½œ ä½  å‚¬ å…¨ å…¬ å‰ åˆ å» å‰ å¯ å–” åœ¨ å ´ å£² å¤œ å¤§ å¥½ å§‹ å®š å· å·» å¹´ æ„› æ æ”¾ æ–° æ—¥ æ˜¯ æ™‚ æœˆ æœ¬ æ£® æ¬¸ æ­£ æµª æµ· ç„¡ ç‰ˆ ç‰¹ ç”Ÿ ç”¨ ç”» ç™º ç™» çœ‹ çœŸ ç¥ ç´„ ç´™ ç´° è‰² è¯ è©³ èª• è±ª è³Š é€ é€† é–‹ é›¶ éŸ³ ê§Ÿ ê°€ ê±° ê²Œ ê³  ê¸° ë‚˜ ë‚¨ ë…„ ë‹ˆ ë‹¤ ë‹¨ ëŒ€ ë” ë“¤ ë¼ ë‘ ë¦¬ ë§ˆ ë©´ ë¬´ ë¯¸ ë¯¼ ë°© ë·” ì‚¬ ìƒ ì„œ ì†Œ ìˆ˜ ìŠˆ ìŠ¤ ì‹œ ì•„ ì•¼ ì–´ ì— ì˜ˆ ìš” ìš° ì€ ì„ ì´ ì¼ ì ì • ì œ ì£¼ ì¤€ ì§€ ì§ íƒ„ íƒœ í•˜ í•œ í•´ í˜• í™‰ ï¸ ğŸ…° ğŸ…´ ğŸ…¸ ğŸ…½ ğŸ…¾ ğŸ†ƒ ğŸŒ§ ğŸ– ğŸ™ ğŸ– ğŸ ğŸŸ ğŸ•´ ğŸ•µ ğŸ–’ ğŸ–¥ ğŸ—“ ğŸ— ğŸ—£ â–e ar â–d â–c mo io â–l as â–de â–p er en us â–a â–s an â–m ario usu usuario @usuario os ji â–emo â–@usuario â–emoji or ta ue on â–h â–t es â–q do â–n â–es in â–U â–que â–la al â–y RL â–URL re un â–v â–en ci is te ara â–has tag htag â–hashtag da â–f â–b â–el â–con ra ic .. to ien â–no â–un ro â–E â–g â–se Ã³n ol â–A â–cara ando â–me il â–C â–S â–M â–o â–P â–ha mp â–por â–r le ti â–L de la ch â–los â–. â–co â–re ia â–lo â–D â–al â–to mos â–per am el â–T â–N ir ja â–in ora Ã­a go ier it â–para ion ab Ã¡s dos ma ciÃ³n â–cu â–si ... â–su â–est lo ente â–las ri â–del â–te â–qu jo â–mi ec â–Y gu ac â–una â–I dad â–ll â–B !! isa â–H â–mu ad res â–llor ba â–le cu cia â–risa qu ca â–J vi tr mb â–Es â–man â–llorando â–Q oy â–an â–i ica ent â–son iendo â–R â–des que â–V co â–\" â–G â–di â–th â–cora AJ ga â–mÃ¡s â–coraz â–1 â–w me â–F era ing di ur â–so â–vi â–j ico ul ues ter â–ch â–pro tra â–pa ada ez tes â–yo â–2 â–No â–mo â–ten ero ce â–ver tos â–como ran â–tien â–ex â–esta â–ser â–pero â–corazÃ³n tas ie iz Ã©n â–O â–as â–po â–ya â–ap Ã¡n tu â–ar ay AJAJ â–par at ios son mi av lar enta olo â–tu ud tar â–El â–ro â–person jos â–pi ver ono cion â–La â–ca ana â–sal pa vo â–ac ve â–tra â–the ten â–( â–fu â–sonr â–pue â–pas idad no se ina tor der jaja per â–Â¿ lan OS â–ta uen cer â–tiene â–todo va ER â–ni cha â–mas ales â–mis so â–hab â–k ido â–sin cho â–pe !!! das â–mano tro dio â–mar â–quier gar â–Que â–res â–comp ig Ã±o â–cas za ens â–ojos 00 â–Me ita â–ab EN â–tan â–En â–ma id â–hay pe men â–va ice â–esto â–sonriendo tan ib acion â–ah â–pre man ON Ã±os min AN Ã­n â–sab ob â–and iÃ³n cias â–cre â–porque ones â–nos AS â–este eces las des mas los â–estÃ¡ ut ado â–can â–piel ?? amil â–tono je ES â–Si â–sus ito â–Yo â–, olvi ores â–he â–cuando â–- â–com â–Se ru AR â–lle po â–os Ã©s ame â–W â–todos mente ici â–bien du â–cor den â–of OR et â–rev â–jo laro â–persona UE â–ra Ã­s les â–famil 19 iste â–3 â–Co â–hacer â–eso â–mej mil â–dÃ­a â–fav zo ena â–pu â–imp â–ent â–fr gun Ãºn oo â–for xico iento â–fuer â–jaja â–buen â–ho â–mal bre â–DE â–favor â–claro â–quÃ© â–pres emos iÃ©n mar â–cos ista don â–Un mpre â–bon â–vida â–ven â–nada â–vo â–dis Ã­as â–ti â–hoy â–fue dose â–traba â–men â–ne â–muy â–ES â–Por ed â–les jer â–pr gan â–De â–Con â–car ill â–gran ces â–amo â–cal ble â–deb â–mejor â–Lo o,'"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "tokenizer_path = \"./sentence-piece-tokenizer\"\n",
    "!mkdir $tokenizer_path\n",
    "vocab_file, merges_file = tokenizer.save_model(tokenizer_path)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "mkdir: cannot create directory â€˜./sentence-piece-tokenizerâ€™: File exists\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "\n",
    "from finetune_vs_scratch.tokenizer import MyTokenizer\n",
    "\n",
    "t_tokenizer = MyTokenizer(\n",
    "    vocab_file,\n",
    "    merges_file,\n",
    ")\n",
    "\n",
    "#sorted(vars(t_tokenizer).keys())"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['_additional_special_tokens',\n",
       " '_bos_token',\n",
       " '_cls_token',\n",
       " '_decode_use_source_tokenizer',\n",
       " '_eos_token',\n",
       " '_mask_token',\n",
       " '_pad_token',\n",
       " '_pad_token_type_id',\n",
       " '_sep_token',\n",
       " '_unk_token',\n",
       " 'added_tokens_decoder',\n",
       " 'added_tokens_encoder',\n",
       " 'bpe_ranks',\n",
       " 'cache',\n",
       " 'decoder',\n",
       " 'deprecation_warnings',\n",
       " 'encoder',\n",
       " 'init_inputs',\n",
       " 'init_kwargs',\n",
       " 'lower',\n",
       " 'merges_file',\n",
       " 'model_input_names',\n",
       " 'model_max_length',\n",
       " 'name_or_path',\n",
       " 'normalization',\n",
       " 'padding_side',\n",
       " 'special_puncts',\n",
       " 'unique_no_split_tokens',\n",
       " 'verbose',\n",
       " 'vocab_file']"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "t_tokenizer.save_pretrained(\"./mytokenizer\")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('./mytokenizer/tokenizer_config.json',\n",
       " './mytokenizer/special_tokens_map.json',\n",
       " './mytokenizer/vocab.txt',\n",
       " './mytokenizer/bpe.codes',\n",
       " './mytokenizer/added_tokens.json')"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "MyTokenizer.from_pretrained(\"./mytokenizer\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "() {'normalization': True, 'bos_token': '<s>', 'eos_token': '</s>', 'sep_token': '</s>', 'cls_token': '<s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'mask_token': '<mask>', 'lower': False, 'special_tokens_map_file': './mytokenizer/special_tokens_map.json', 'tokenizer_file': None, 'name_or_path': './mytokenizer'}\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "__init__() missing 2 required positional arguments: 'vocab_file' and 'merges_file'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_365518/2240273569.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mMyTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./mytokenizer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/finetune-vs-scratch-gHiQbun3-py3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1716\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1717\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"loading file {file_path} from cache at {resolved_vocab_files[file_id]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1718\u001b[0;31m         return cls._from_pretrained(\n\u001b[0m\u001b[1;32m   1719\u001b[0m             \u001b[0mresolved_vocab_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_configuration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minit_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1720\u001b[0m         )\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/finetune-vs-scratch-gHiQbun3-py3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1790\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1791\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1792\u001b[0;31m             \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minit_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1793\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1794\u001b[0m             raise OSError(\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 2 required positional arguments: 'vocab_file' and 'merges_file'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "QuÃ© garcha pasa?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tugo = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\")\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "() {'model_max_length': 128, 'vocab_file': '/home/jmperez/.cache/huggingface/transformers/9a877d0d57efbfeae96fec396a35595dc8c4685fe2b7b2049c6c094e24a0e8bf.f8a4dfe5c3c45a26f9df849d732decb191dc0c05ab270799695430332d143982', 'merges_file': '/home/jmperez/.cache/huggingface/transformers/1c2d05a06ac61a063ad62a7590731a28cc62f58e2802c76b5f993165f25894a9.75877d86011e5d5d46614d3a21757b705e9d20ed45a019805d25159b4837b0a4', 'special_tokens_map_file': None, 'tokenizer_file': None, 'name_or_path': 'vinai/bertweet-base'}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "\n",
    "tugo.vocab_file"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/home/jmperez/.cache/huggingface/transformers/9a877d0d57efbfeae96fec396a35595dc8c4685fe2b7b2049c6c094e24a0e8bf.f8a4dfe5c3c45a26f9df849d732decb191dc0c05ab270799695430332d143982'"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "\n",
    "tugo.save_pretrained(\"./pepe\")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('./pepe/tokenizer_config.json',\n",
       " './pepe/special_tokens_map.json',\n",
       " './pepe/vocab.txt',\n",
       " './pepe/bpe.codes',\n",
       " './pepe/added_tokens.json')"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## WordPiece"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "\n",
    "\n",
    "tokenizer = BertWordPieceTokenizer(\n",
    "    clean_text=True,\n",
    "    handle_chinese_chars=True,\n",
    "    strip_accents=False,\n",
    "    lowercase=False,\n",
    ")\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "from finetune_vs_scratch.preprocessing import special_tokens\n",
    "\n",
    "tokenizer.add_special_tokens(special_tokens)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "tokenizer.train_from_iterator(\n",
    "    tweets,\n",
    "    vocab_size=40_000,\n",
    "    min_frequency=2,\n",
    "    show_progress=True,\n",
    "\n",
    "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"] + special_tokens,\n",
    "    limit_alphabet=600,\n",
    "    wordpieces_prefix=\"##\",\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "!mkdir test_tokenizer\n",
    "tokenizer.save_model(\"./test_tokenizer/\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "mkdir: cannot create directory â€˜test_tokenizerâ€™: File exists\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['./test_tokenizer/vocab.txt']"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Reload"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "from transformers import BertTokenizer, BertTokenizerFast\n",
    "\n",
    "bert_tokenizer = BertTokenizerFast.from_pretrained(\n",
    "    \"./test_tokenizer/\",\n",
    "    never_split=special_tokens,\n",
    ")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading trained tokenizers"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "\n",
    "!pip freeze | grep transf"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[33mWARNING: Could not generate requirement for distribution -orch 1.9.0+cu111 (/home/jmperez/.cache/pypoetry/virtualenvs/finetune-vs-scratch-gHiQbun3-py3.8/lib/python3.8/site-packages): Parse error at \"'-orch==1'\": Expected W:(abcd...)\u001b[0m\n",
      "transformers @ file:///home/jmperez/.cache/pypoetry/artifacts/d7/d1/6f/c08a86d09ebb99ef7233126f12dce131f0bcf38f23b1c8aa8d2065c528/transformers-4.8.2-py3-none-any.whl\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "from transformers import BertTokenizerFast, AutoTokenizer, BertTokenizer\n",
    "from finetune_vs_scratch.model import load_tokenizer\n",
    "\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\n",
    "    \"../models/tokenizers/betito_cased_accents/\",\n",
    "    never_split=[\"@usuario\"]\n",
    ")\n",
    "\n",
    "\n",
    "bert_tokenizer(\"@usuario\")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'input_ids': [2, 5, 3], 'token_type_ids': [0, 0, 0], 'attention_mask': [1, 1, 1]}"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Si es Fast, le tenemos que poner `additional_special_tokens`, LTA"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "from transformers import BertTokenizerFast, AutoTokenizer, BertTokenizer\n",
    "from finetune_vs_scratch.model import load_tokenizer\n",
    "\n",
    "bert_tokenizer = BertTokenizerFast.from_pretrained(\n",
    "    \"../models/tokenizers/betito_cased_accents/\",\n",
    "    additional_special_tokens=[\"@usuario\"]\n",
    ")\n",
    "\n",
    "\n",
    "bert_tokenizer(\"@usuario\")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'input_ids': [2, 5, 3], 'token_type_ids': [0, 0, 0], 'attention_mask': [1, 1, 1]}"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "\n",
    "bert_tokenizer(\"@usuario\")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'input_ids': [2, 38, 1164, 3], 'token_type_ids': [0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1]}"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import re \n",
    "\n",
    "tweet = \"@usuario @usuario jajaja gil emoji riendo emoji emoji riendo emoji\"\n",
    "\n",
    "tweet = re.sub(\"emoji.*?emoji\", \"emoji\", tweet)\n",
    "print(tweet)\n",
    "tokens = bert_tokenizer(tweet)[\"input_ids\"]\n",
    "print(tokens)\n",
    "print(bert_tokenizer.decode(tokens))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "@usuario @usuario jajaja gil emoji emoji\n",
      "[2, 38, 1164, 38, 1164, 5733, 621, 5411, 8, 8, 3]\n",
      "[CLS] @ usuario @ usuario jajaja gil emoji emoji [SEP]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', use_fast=False, do_lower_case=True, do_basic_tokenize=True)\n",
    "tokenizer.add_tokens(['graft', 'grafts'])"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a4bd9500643240529c87f4d39be57585"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e43bf239ef5145f1a3a49fba534210bc"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2a902248c7b547af94768d8d17bb785e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "tokenizer.convert_ids_to_tokens(tokenizer.encode(\"graft\"))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['[CLS]', 'graft', '[SEP]']"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('finetune-vs-scratch-gHiQbun3-py3.8': poetry)"
  },
  "interpreter": {
   "hash": "28c1932dff7617228923490e32f133f79d588eb74ca6c2b1f196ab0fdc858ed2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}