{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Tokenization\n",
    "\n",
    "### BertTweet\n",
    "\n",
    "- fastBPE\n",
    "- 64K subword\n",
    "\n",
    "### Twilbert\n",
    "- SentencePiece (fastBPE)\n",
    "- 30k subword "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from glob import glob\n",
    "\n",
    "num_files = 100\n",
    "tweet_files = glob(\"../../data/filtered_tweets/*.txt\")\n",
    "\n",
    "train_files = tweet_files[:2]\n",
    "\n",
    "\n",
    "tweets = list([x.strip(\"\\n\") for x in open(tweet_files[0])])[:1_000_000]"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "source": [
    "len(tweets)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "metadata": {},
     "execution_count": 77
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "source": [
    "\n",
    "SentencePieceBPETokenizer().normalizer"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tokenizers.normalizers.NFKC at 0x7f792b1e6770>"
      ]
     },
     "metadata": {},
     "execution_count": 111
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "source": [
    "from tokenizers import SentencePieceUnigramTokenizer, SentencePieceBPETokenizer, BertWordPieceTokenizer, ByteLevelBPETokenizer\n",
    "from tokenizers import normalizers \n",
    "\n",
    "tokenizer = SentencePieceBPETokenizer()\n",
    "#replacement=\"_\")\n",
    "\n",
    "\n",
    "tokenizer.normalizer = normalizers.Sequence([\n",
    "    normalizers.Lowercase(),\n",
    "    normalizers.NFKC()\n",
    "])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "source": [
    "from finetune_vs_scratch.preprocessing import special_tokens\n",
    "from finetune_vs_scratch.tokenizer import tokenizer_special_tokens\n",
    "\n",
    "#tokenizer.add_special_tokens(tokenizer_special_tokens)\n",
    "tokenizer.train_from_iterator(\n",
    "    tweets,\n",
    "    vocab_size=30_000,\n",
    "    min_frequency=,\n",
    "    show_progress=True,\n",
    "    limit_alphabet=300,\n",
    "    special_tokens=tokenizer_special_tokens + special_tokens,\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Alphabet"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "source": [],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'! \" # $ % & \\' ( ) * + , - . / 0 1 2 3 4 5 6 7 8 9 : ; < = > ? @ A E I N O S [ \\\\ ] ^ _ ` a b c d e f g h i j k l m n o p q r s t u v w x y z { | } Â¡ Â¬ Â¿ Ã  Ã¡ Ã¢ Ã£ Ã¤ Ã§ Ã¨ Ã© Ãª Ã¬ Ã­ Ã® Ã¯ Ã± Ã² Ã³ Ã´ Ãµ Ã¶ Ã¹ Ãº Ã¼ Ä‡ Ä Ä‘ ÄŸ Ä± ÅŸ Å¡ Å¾ Éª É´ Ëˆ Ì„ Ì‡ Ì¶ Ğ° Ğ² Ğ´ Ğµ Ğ¸ Ğº Ğ» Ğ¼ Ğ½ Ğ¾ Ñ€ Ñ Ñ‚ Ñƒ ØŒ ØŸ Ø¡ Ø¢ Ø£ Ø¤ Ø¥ Ø¦ Ø§ Ø¨ Ø© Øª Ø« Ø¬ Ø­ Ø® Ø¯ Ø° Ø± Ø² Ø³ Ø´ Øµ Ø¶ Ø· Ø¸ Ø¹ Øº Ù€ Ù Ù‚ Ùƒ Ù„ Ù… Ù† Ù‡ Ùˆ Ù‰ ÙŠ Ù‹ Ù Ù Ù Ù Ù‘ Ù’ Ù° Ù¹ Ù¾ Ú† Ú‘ Ú© Ú¯ Úº Ú¾ Û Ûƒ ÛŒ Û’ Û” \\u06dd Û¡ à¤‚ à¤• à¤— à¤œ à¤¤ à¤¦ à¤¨ à¤ª à¤¬ à¤® à¤¯ à¤° à¤² à¤¸ à¤¹ à¤¼ à¤¾ à¤¿ à¥€ à¥ à¥‡ à¥ˆ à¥‹ à¥ à¸ à¸„ à¸‡ à¸” à¸• à¸— à¸™ à¸š à¸¡ à¸¢ à¸£ à¸¥ à¸§ à¸« à¸­ à¸± à¸² à¸µ à¹€ à¹ à¹ˆ à¹‰ á„ á…² á´€ á´‡ á´ \\u200b \\u200d \\u2060 \\u2063 \\u2066 \\u2069 âƒ¢ âƒ£ â†’ â” â”ƒ â”“ â”— â”› â– â–ˆ â–ª â–¶ â–º â˜€ â˜… â˜† â˜‡ â˜‘ â™ª â™« âš’ âš˜ âš  â› âœ âœ” â£ â¡ â½ â € â¬‡ ã€ ã€‚ ã€Œ ã€ ã‚ ã„ ã† ãŠ ã‹ ãŒ ã ã ã‘ ã“ ã• ã— ã™ ãŸ ã  ã¡ ã£ ã¤ ã¦ ã§ ã¨ ãª ã« ã® ã¯ ã¾ ã‚‚ ã‚ƒ ã‚ˆ ã‚‰ ã‚Š ã‚‹ ã‚Œ ã‚’ ã‚“ ã‚¢ ã‚¤ ã‚© ã‚¯ ã‚³ ã‚· ã‚¸ ã‚¹ ã‚¿ ãƒ ãƒƒ ãƒ„ ãƒˆ ãƒ‹ ãƒ ãƒ ãƒ• ãƒ— ãƒ ãƒ£ ãƒ© ãƒª ãƒ« ãƒ¬ ãƒ­ ãƒ³ ãƒ» ä¸€ ä¸ ä¹ˆ äº† äºº ä½  åš å¯ å“ˆ å•Š å¤§ å¥½ æˆ‘ æ æ—¥ æ˜¯ æœ€ æœ‰ æœ¬ çˆ± ç‹ çš„ çœŸ ê°€ ê±° ê²Œ ê²° ê³  êµ­ ê·¸ ê·¼ ê¸° ê¹€ ë‚˜ ë‚¨ ë‚´ ë„ˆ ë…„ ëŠ” ëŠ˜ ë‹ˆ ë‹¤ ë‹¨ ëŒ€ ë” ë° ë„ ë™ ë‘ ë“œ ë“  ë“¤ ë¼ ë‘ ëŸ½ ë ˆ ë¡œ ë¥¼ ë¦¬ ë§ˆ ë§Œ ëª¨ ëª¬ ë¬´ ë¯¸ ë¯¼ ë°• ë°© ë°± ë²ˆ ë³´ ë¶ˆ ë¸ ë¹„ ì‚¬ ìƒ ìƒ ì„œ ì„ ì„± ì„¸ ì†Œ ìˆ˜ ìŠ¤ ì‹œ ì•„ ì•ˆ ì•¼ ì–´ ì— ì—‘ ì—” ì—¬ ì—´ ì˜ ì˜ˆ ì˜¤ ìš” ìš° ìš´ ì›Œ ì› ìœ  ìœ¼ ì€ ì„ ìŒ ì˜ ì´ ì¸ ì¼ ìˆ ì ì‘ ì • ì œ ì£¼ ì¤€ ì¤‘ ì¦ˆ ì§€ ì§ ì§„ ì°¨ ì°¬ ì¶• ì¹´ ì¼€ í¬ í´ í‚¤ íƒ€ íƒ„ íƒœ íˆ¬ íŠ¸ í‹° íŒ¬ í¬ í”¼ í•˜ í•œ í•¨ í•´ í˜„ í˜¸ í™‰ í™” í›ˆ ğŸŒ§ ğŸ½ ğŸ™ ğŸ– ğŸŸ ğŸ‘ ğŸ“½ ğŸ•³ ğŸ—“ ğŸ— ğŸ—£'"
      ]
     },
     "metadata": {},
     "execution_count": 109
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "source": [
    "tokenizer.encode(\"@usuario son UNA MIERDA\").tokens"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['â–sonr', 'den', 'â–claro', 'â–arri']"
      ]
     },
     "metadata": {},
     "execution_count": 85
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "transformer_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tokenizer\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "source": [],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'@usuario tugo bierno skere comunista'"
      ]
     },
     "metadata": {},
     "execution_count": 113
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "transformer_tokenizer.save_pretrained"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "source": [
    "normalizer = normalizers.BertNormalizer(clean_text=True, strip_accents=True)\n",
    "\n",
    "normalizer.normalize_str(\"Ã¡laba\")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'alaba'"
      ]
     },
     "metadata": {},
     "execution_count": 119
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('finetune-vs-scratch-gHiQbun3-py3.8': poetry)"
  },
  "interpreter": {
   "hash": "28c1932dff7617228923490e32f133f79d588eb74ca6c2b1f196ab0fdc858ed2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}