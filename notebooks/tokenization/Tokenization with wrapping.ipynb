{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Tokenization\n",
    "\n",
    "### BertTweet\n",
    "\n",
    "- fastBPE\n",
    "- 64K subword\n",
    "\n",
    "### Twilbert\n",
    "- SentencePiece (fastBPE)\n",
    "- 30k subword "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from glob import glob\n",
    "\n",
    "num_files = 100\n",
    "tweet_files = glob(\"../../data/filtered_tweets/*.txt\")\n",
    "\n",
    "train_files = tweet_files[:2]\n",
    "\n",
    "\n",
    "tweets = list([x.strip(\"\\n\") for x in open(tweet_files[0])])[:1_000_000]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "len(tweets)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "from tokenizers.processors import TemplateProcessing\n",
    "\n",
    "TemplateProcessing?"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m \u001b[0mTemplateProcessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msingle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpair\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspecial_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "Provides a way to specify templates in order to add the special tokens to each\n",
      "input sequence as relevant.\n",
      "\n",
      "Let's take :obj:`BERT` tokenizer as an example. It uses two special tokens, used to\n",
      "delimitate each sequence. :obj:`[CLS]` is always used at the beginning of the first\n",
      "sequence, and :obj:`[SEP]` is added at the end of both the first, and the pair\n",
      "sequences. The final result looks like this:\n",
      "\n",
      "    - Single sequence: :obj:`[CLS] Hello there [SEP]`\n",
      "    - Pair sequences: :obj:`[CLS] My name is Anthony [SEP] What is my name? [SEP]`\n",
      "\n",
      "With the type ids as following::\n",
      "\n",
      "    [CLS]   ...   [SEP]   ...   [SEP]\n",
      "      0      0      0      1      1\n",
      "\n",
      "You can achieve such behavior using a TemplateProcessing::\n",
      "\n",
      "    TemplateProcessing(\n",
      "        single=\"[CLS] $0 [SEP]\",\n",
      "        pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
      "        special_tokens=[(\"[CLS]\", 1), (\"[SEP]\", 0)],\n",
      "    )\n",
      "\n",
      "In this example, each input sequence is identified using a ``$`` construct. This identifier\n",
      "lets us specify each input sequence, and the type_id to use. When nothing is specified,\n",
      "it uses the default values. Here are the different ways to specify it:\n",
      "\n",
      "    - Specifying the sequence, with default ``type_id == 0``: ``$A`` or ``$B``\n",
      "    - Specifying the `type_id` with default ``sequence == A``: ``$0``, ``$1``, ``$2``, ...\n",
      "    - Specifying both: ``$A:0``, ``$B:1``, ...\n",
      "\n",
      "The same construct is used for special tokens: ``<identifier>(:<type_id>)?``.\n",
      "\n",
      "**Warning**: You must ensure that you are giving the correct tokens/ids as these\n",
      "will be added to the Encoding without any further check. If the given ids correspond\n",
      "to something totally different in a `Tokenizer` using this `PostProcessor`, it\n",
      "might lead to unexpected results.\n",
      "\n",
      "Args:\n",
      "    single (:obj:`Template`):\n",
      "        The template used for single sequences\n",
      "\n",
      "    pair (:obj:`Template`):\n",
      "        The template used when both sequences are specified\n",
      "\n",
      "    special_tokens (:obj:`Tokens`):\n",
      "        The list of special tokens used in each sequences\n",
      "\n",
      "Types:\n",
      "\n",
      "    Template (:obj:`str` or :obj:`List`):\n",
      "        - If a :obj:`str` is provided, the whitespace is used as delimiter between tokens\n",
      "        - If a :obj:`List[str]` is provided, a list of tokens\n",
      "\n",
      "    Tokens (:obj:`List[Union[Tuple[int, str], Tuple[str, int], dict]]`):\n",
      "        - A :obj:`Tuple` with both a token and its associated ID, in any order\n",
      "        - A :obj:`dict` with the following keys:\n",
      "            - \"id\": :obj:`str` => The special token id, as specified in the Template\n",
      "            - \"ids\": :obj:`List[int]` => The associated IDs\n",
      "            - \"tokens\": :obj:`List[str]` => The associated tokens\n",
      "\n",
      "         The given dict expects the provided :obj:`ids` and :obj:`tokens` lists to have\n",
      "         the same length.\n",
      "\u001b[0;31mFile:\u001b[0m           ~/.cache/pypoetry/virtualenvs/finetune-vs-scratch-gHiQbun3-py3.8/lib/python3.8/site-packages/tokenizers/processors/__init__.py\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\u001b[0;31mSubclasses:\u001b[0m     \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "from tokenizers import SentencePieceBPETokenizer, BertWordPieceTokenizer, ByteLevelBPETokenizer\n",
    "from tokenizers import normalizers \n",
    "from tokenizers.processors import RobertaProcessing\n",
    "from finetune_vs_scratch.preprocessing import special_tokens\n",
    "from finetune_vs_scratch.tokenizer import tokenizer_special_tokens\n",
    "\n",
    "tokenizer = SentencePieceBPETokenizer()\n",
    "#replacement=\"_\")\n",
    "\n",
    "strip_accents = True\n",
    "lowercase = True\n",
    "tokenizer.add_special_tokens(tokenizer_special_tokens)\n",
    "\n",
    "tokenizer_normalizers = [\n",
    "    normalizers.NFKC(),\n",
    "    normalizers.BertNormalizer(\n",
    "        clean_text=True,\n",
    "        handle_chinese_chars=True,\n",
    "        strip_accents=strip_accents,\n",
    "        lowercase=lowercase,\n",
    "    )\n",
    "]\n",
    "\n",
    "tokenizer.normalizer = normalizers.Sequence(tokenizer_normalizers)\n",
    "\n",
    "vocab = tokenizer.get_vocab()\n",
    "\n",
    "tokenizer.post_processor = RobertaProcessing(\n",
    "    cls=(\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    "    sep=(\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "from finetune_vs_scratch.preprocessing import special_tokens\n",
    "from finetune_vs_scratch.tokenizer import tokenizer_special_tokens\n",
    "\n",
    "#tokenizer.add_special_tokens(tokenizer_special_tokens)\n",
    "tokenizer.train_from_iterator(\n",
    "    tweets,\n",
    "    vocab_size=30_000,\n",
    "    min_frequency=5,\n",
    "    show_progress=True,\n",
    "    limit_alphabet=300,\n",
    "    special_tokens=tokenizer_special_tokens + special_tokens,\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Alphabet"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "vocab = tokenizer.get_vocab()\n",
    "\n",
    "inv_vocab = {v:k for k, v in vocab.items()}\n",
    "inv_vocab = [inv_vocab[i] for i in range(len(inv_vocab))]\n",
    "\n",
    "print(f\"First tokens: {inv_vocab[:200]}\")\n",
    "\n",
    "alphabet = sorted(list({a for x in tokenizer.get_vocab() for a in x}))\n",
    "print(\"Alphabet = \", \" \".join(alphabet))\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "First tokens: ['<s>', '<pad>', '</s>', '<unk>', '<mask>', '@usuario', 'url', 'hashtag', 'emoji', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', '@', '[', '\\\\', ']', '^', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '|', '}', '¬°', '¬¨', '¬ø', 'ƒ±', '…™', '–∞', '–µ', '–∏', '–∫', '–Ω', '–æ', '—Å', '—Ç', 'ÿå', 'ÿ°', 'ÿß', 'ÿ®', 'ÿ©', 'ÿ™', 'ÿ´', 'ÿ¨', 'ÿ≠', 'ÿÆ', 'ÿØ', 'ÿ∞', 'ÿ±', 'ÿ≤', 'ÿ≥', 'ÿ¥', 'ÿµ', 'ÿ∂', 'ÿ∑', 'ÿ∏', 'ÿπ', 'ÿ∫', 'ŸÄ', 'ŸÅ', 'ŸÇ', 'ŸÉ', 'ŸÑ', 'ŸÖ', 'ŸÜ', 'Ÿá', 'Ÿà', 'Ÿâ', 'Ÿä', 'Ÿπ', 'Ÿæ', '⁄Ü', '⁄ë', '⁄©', '⁄Ø', '⁄∫', '⁄æ', '€Å', '€É', '€å', '€í', '€î', '‡§ï', '‡§ú', '‡§§', '‡§¶', '‡§®', '‡§™', '‡§¨', '‡§Æ', '‡§Ø', '‡§∞', '‡§≤', '‡§∏', '‡§π', '‡§æ', '‡§ø', '‡•Ä', '‡•ã', '‡∏Å', '‡∏Ñ', '‡∏á', '‡∏î', '‡∏ô', '‡∏°', '‡∏¢', '‡∏£', '‡∏•', '‡∏ß', '‡∏≠', '‡∏≤', '‡πÄ', '·ÑÄ', '·ÑÅ', '·ÑÇ', '·ÑÉ', '·ÑÑ', '·ÑÖ', '·ÑÜ', '·Ñá', '·Ñâ', '·Ñä', '·Ñã', '·Ñå', '·Ñç', '·Ñé', '·Ñè', '·Ñê', '·Ñë', '·Ñí', '·Ö°', '·Ö¢', '·Ö£', '·Ö•', '·Ö¶', '·Öß', '·Ö®', '·Ö©', '·Ö™', '·Ö¨', '·Ö≠', '·ÖÆ', '·ÖØ', '·Ö±', '·Ö≤', '·Ö≥', '·Ö¥', '·Öµ', '·Ü®', '·Ü´']\n",
      "Alphabet =  ! \" # $ % & ' ( ) * + , - . / 0 1 2 3 4 5 6 7 8 9 : ; < = > ? @ [ \\ ] ^ _ ` a b c d e f g h i j k l m n o p q r s t u v w x y z | } ¬° ¬¨ ¬ø ƒ± …™ –∞ –µ –∏ –∫ –Ω –æ —Å —Ç ÿå ÿ° ÿß ÿ® ÿ© ÿ™ ÿ´ ÿ¨ ÿ≠ ÿÆ ÿØ ÿ∞ ÿ± ÿ≤ ÿ≥ ÿ¥ ÿµ ÿ∂ ÿ∑ ÿ∏ ÿπ ÿ∫ ŸÄ ŸÅ ŸÇ ŸÉ ŸÑ ŸÖ ŸÜ Ÿá Ÿà Ÿâ Ÿä Ÿπ Ÿæ ⁄Ü ⁄ë ⁄© ⁄Ø ⁄∫ ⁄æ €Å €É €å €í €î ‡§ï ‡§ú ‡§§ ‡§¶ ‡§® ‡§™ ‡§¨ ‡§Æ ‡§Ø ‡§∞ ‡§≤ ‡§∏ ‡§π ‡§æ ‡§ø ‡•Ä ‡•ã ‡∏Å ‡∏Ñ ‡∏á ‡∏î ‡∏ô ‡∏° ‡∏¢ ‡∏£ ‡∏• ‡∏ß ‡∏≠ ‡∏≤ ‡πÄ ·ÑÄ ·ÑÅ ·ÑÇ ·ÑÉ ·ÑÑ ·ÑÖ ·ÑÜ ·Ñá ·Ñâ ·Ñä ·Ñã ·Ñå ·Ñç ·Ñé ·Ñè ·Ñê ·Ñë ·Ñí ·Ö° ·Ö¢ ·Ö£ ·Ö• ·Ö¶ ·Öß ·Ö® ·Ö© ·Ö™ ·Ö¨ ·Ö≠ ·ÖÆ ·ÖØ ·Ö± ·Ö≤ ·Ö≥ ·Ö¥ ·Öµ ·Ü® ·Ü´ ·ÜØ ·Ü∑ ·Ü∏ ·Ü∫ ·Üª ·Üº ·¥Ä ·¥á ·¥è ‚É£ ‚Üí ‚îÅ ‚îÉ ‚îì ‚ñÅ ‚ñà ‚ñ∂ ‚ñ∫ ‚òÄ ‚òÖ ‚òÜ ‚òá ‚ô™ ‚öí ‚öò ‚ö† ‚õè ‚úî ‚ù£ ‚û° ‚†Ä ‚¨á „ÄÅ „ÄÇ „Äå „Äç „ÅÇ „ÅÑ „ÅÜ „Åä „Åã „Åç „Åè „Åë „Åì „Åï „Åó „Åô „Åü „Å° „Å£ „Å§ „Å¶ „Å® „Å™ „Å´ „ÅÆ „ÅØ „Åæ „ÇÇ „ÇÉ „Çà „Çâ „Çä „Çã „Çå „Çí „Çì „Ç¢ „Ç§ „ÇØ „Ç≥ „Ç∑ „Çπ „Çø „ÉÉ „ÉÜ „Éà „Éè „Éí „Éï „É© „É™ „É´ „É¨ „É≠ „É≥ „Éª ‰∏Ä ‰∏ç ‰∫Ü ‰∫∫ ‰Ω† Âçö ÂèØ Âìà Âïä Â•Ω Êàë Êó• ÊòØ Áéã ÁöÑ üåß üéô üëÅ üìΩ üï≥ üó£\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "tokenizer.encode(\"@usuario son UNA MIERDA\", \"Viva Per√≥n\").tokens"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['<s>',\n",
       " '@usuario',\n",
       " '‚ñÅson',\n",
       " '‚ñÅuna',\n",
       " '‚ñÅmierda',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '‚ñÅviva',\n",
       " '‚ñÅperon',\n",
       " '</s>']"
      ]
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "transformer_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tokenizer,\n",
    "    bos_token=\"<s>\",\n",
    "    eos_token=\"</s>\",\n",
    "    sep_token=\"</s>\",\n",
    "    cls_token=\"<s>\",\n",
    "    unk_token=\"<unk>\",\n",
    "    pad_token=\"<pad>\",\n",
    "    mask_token=\"<mask>\",\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "\n",
    "transformer_tokenizer.save_pretrained(\"small\")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('small/tokenizer_config.json',\n",
       " 'small/special_tokens_map.json',\n",
       " 'small/tokenizer.json')"
      ]
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "from transformers import AutoTokenizer\n",
    "transformer_tokenizer = AutoTokenizer.from_pretrained(\"small\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "() {'bos_token': '<s>', 'eos_token': '</s>', 'sep_token': '</s>', 'cls_token': '<s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'mask_token': '<mask>', 'special_tokens_map_file': 'small/special_tokens_map.json', 'tokenizer_file': 'small/tokenizer.json', 'name_or_path': 'small'}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "source": [
    "transformer_tokenizer._tokenizer.encode(\"Este es un forro @usuario impresion√°nte\", \"Corte gil corte basura\").tokens\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['<s>',\n",
       " '‚ñÅeste',\n",
       " '‚ñÅes',\n",
       " '‚ñÅun',\n",
       " '‚ñÅforro',\n",
       " '‚ñÅ',\n",
       " '@usuario',\n",
       " '‚ñÅimpresionante',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '‚ñÅcorte',\n",
       " '‚ñÅgil',\n",
       " '‚ñÅcorte',\n",
       " '‚ñÅbasura',\n",
       " '</s>']"
      ]
     },
     "metadata": {},
     "execution_count": 65
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "() {'model_max_length': 512, 'vocab_file': '/home/jmperez/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab', 'merges_file': '/home/jmperez/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b', 'tokenizer_file': '/home/jmperez/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730', 'special_tokens_map_file': None, 'name_or_path': 'roberta-base'}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "source": [
    "\n",
    "tokenizer._tokenizer.encode(\"Oh man this is terrible\", \"Bullshit\").tokens"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['<s>',\n",
       " 'Oh',\n",
       " 'ƒ†man',\n",
       " 'ƒ†this',\n",
       " 'ƒ†is',\n",
       " 'ƒ†terrible',\n",
       " '</s>',\n",
       " '</s>',\n",
       " 'Bull',\n",
       " 'shit',\n",
       " '</s>']"
      ]
     },
     "metadata": {},
     "execution_count": 64
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('finetune-vs-scratch-gHiQbun3-py3.8': poetry)"
  },
  "interpreter": {
   "hash": "28c1932dff7617228923490e32f133f79d588eb74ca6c2b1f196ab0fdc858ed2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}